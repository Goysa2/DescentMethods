export Shamanskii_LS

"""
A globalized Shamanskii algorithm with Line Search.
This code is an implementation of the idea presented in:

    Global Convergence Technique for the Newton
        Method with Periodic Hessian Evaluation

By F. LAMPARIELLO and M. SCIANDRONE

JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS:
Vol. 111, No. 2, pp. 341‚Äì358, November 2001
"""
function Shamanskii_LS(nlp            :: AbstractNLPModel,
                       nlp_stop       :: NLPStopping;
                       linesearch     :: Function = shamanskii_line_search,
                       verbose        :: Bool = false,
                       Nwtdirection   :: Function = NwtdirectionLDLt,
                       hessian_rep    :: Function = hessian_dense,
                       mem            :: Int = 2,
                       Œ∑              :: Float64 = 1.5, # Œ∑ > 1
                       c‚Çê             :: Float64 = 0.1, # c‚Çê ‚àà (0, 1]
                       cùêü              :: Float64 = 0.25, # cùêü ‚àà (0, 1)
                       p              :: Int = 10, # p > 1
                       kwargs...)

    nlp_at_x = nlp_stop.current_state

    L = nothing; Q = nothing; Œì = nothing
    D = nothing; pp = nothing; rho = nothing; ncomp = nothing;
    Œ±‚Çê = 0.0; Œ±‚Çõ = 0.0;

    x = copy(nlp.meta.x0) # our x‚ÇÄ
    n = nlp.meta.nvar

    # xt = Array{Float64}(n)
    x‚Çñ = copy(nlp.meta.x0)
    ‚àáf‚Çñ = Array{Float64}(undef, n)

    f = obj(nlp, x)
    f‚Çñ‚Çã‚ÇÅ = f
    ‚àáf‚Çñ = grad(nlp, x)

    # Initiliazation to avoir scope issues
    H‚Çñ = nothing; H‚Çñ‚Çã‚ÇÅ = nothing; approx_H‚Çñ = nothing; approx_H‚Çñ‚Çã‚ÇÅ = nothing;
    # Step 0 of the algorithm, initialize some parameters
    k = 0; i = 0; u = 0

    # Step 1 of the algorithm, we check if the initial point is stationnary
    OK = update_and_start!(nlp_stop, x = x, fx = f, gx = ‚àáf‚Çñ, g0 = ‚àáf‚Çñ)


    ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
    # update!(nlp_at_x, Hx = hessian_rep(nlp, x))
    # H‚Çñ‚Çã‚ÇÅ = nlp_at_x.Hx

    verbose && @printf("%4s  %8s  %7s  %8s  %5s  %5s  %6s  %6s \n", " k", "f", "‚Äñ‚àáf‚Äñ", "k", "i", "p", "Œ±‚Çê", "Œ±‚Çõ")
    verbose && @printf("%5d  %9.2e  %8.1e  %5d  %5d  %5d  %7.2e  %7.2e", k, nlp_at_x.fx, ‚àáfNorm, k, i, p, Œ±‚Çê, Œ±‚Çõ)

    d = fill(0.0, size(nlp_at_x.gx)[1])


    h = LineModel(nlp, x, d)

    # Step 1 : We check if we have a stationnary point
    while !OK #k < 10
        # Step 2 : We compute the exact hessian
        if (k == i * p) || (u == 1)   # !((k != i * p) && (u == 0))
            # println("on est dans le if (k == i * p) || (u == 1)")
            # Compute H‚Çñ and construct its positive definite approximation
            # by applying the modified Cholesky factorization. Then u = 0 or
            # u = 1 depending on wheter or not ÃÇH‚Çñ is to different fom H‚Çñ.
            H‚Çñ = hessian_rep(nlp, x‚Çñ)
            update!(nlp_at_x, Hx = H‚Çñ)

            (L, D, pp, rho, ncomp) = ldlt_symm(H‚Çñ)
            Œî, Q = eigen(Symmetric(D))

            œµ2 =  1.0e-8
            Œì = max.(abs.(Œî),œµ2)
            # sizeD = size(D)[1]^2
            # for j = 1:sizeD
            #     dj = D[j]
            #     D[j] = max(dj, Œì)
            # end


            # println("on check si on a une bonne approximation de la hessienne avec Cholesky modifi√©")
            # println("isposdef(H‚Çñ) = $(isposdef(H‚Çñ))")
            good_hess_approx = hess_approx(H‚Çñ, L, D, Œ∑)
            # println("k = $k and good_hess_approx = $good_hess_approx")
            if good_hess_approx
                u = 0
            else
                u = 1
            end

            # println("i = $i and p = $p")
            if (k == i * p)
                # printstyled("on augmente i = $i \n", color = :green)
                i += 1
            end
        end # if (k == i * p) || (u == 1)

        # Step 3 : ???
        # ÃÇH‚Çñ = ÃÇH‚Çñ‚Çã‚ÇÅ

        # Step 4: compute the descent direction and x‚Çñ‚Çä‚ÇÅ
        # d = Nwtdirection(nlp_at_x.Hx, nlp_at_x.gx, verbose = false)
        dÃÉ = L\nlp_at_x.gx[pp]
        dÃÇ = L'\ (Q*(Q'*dÃÉ ./ Œì))
        d = - dÃÇ[invperm(pp)]

        slope = BLAS.dot(n, d, 1, nlp_at_x.gx, 1)

        # verbose && @printf("  %8.1e", slope)

        h = redirect!(h, x‚Çñ, d)

        # ls_at_t = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
        ls_at_t1 = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
        ls_at_t2 = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
        stop_ls1 = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t1)
        stop_ls2 = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t2)
        verbose && println(" ")
        # ls_at_t, good_step_size = linesearch(h, stop_ls, LS_Function_Meta())
        ls_at_t1, good_step_size1 = shamanskii_line_search(h, stop_ls1, LS_Function_Meta())
        ls_at_t2, good_step_size2 = armijo_ls(h, stop_ls2, LS_Function_Meta())
        good_step_size1 || (nlp_stop.meta.stalled_linesearch = true)

        Œ±‚Çõ = ls_at_t1.x
        Œ±‚Çê = ls_at_t2.x
        # println("good_step_size = $good_step_size  and Œ±‚Çñ = $Œ±‚Çñ")

        # x‚Çñ = nlp_at_x.x + Œ±‚Çñ * d
        x‚Çñ = nlp_at_x.x + Œ±‚Çõ * d
        f‚Çñ = obj(nlp, x‚Çñ); ‚àáf‚Çñ = grad(nlp, x‚Çñ)

        BLAS.blascopy!(n, nlp_at_x.x, 1, x‚Çñ, 1)
        BLAS.axpy!(n, ls_at_t1.x, d, 1, x‚Çñ, 1) #BLAS.axpy!(n, t, d, 1, xt, 1)
        ‚àáf‚Çñ = grad!(nlp, x‚Çñ, ‚àáf‚Çñ)

        # Step 5: We update the value of u
        # if (Œ±‚Çñ >= c‚Çê) || (((f‚Çñ - f‚Çñ‚Çã‚ÇÅ)/abs(f‚Çñ)) >= cùêü)
        if (Œ±‚Çõ >= c‚Çê) || (((f‚Çñ - f‚Çñ‚Çã‚ÇÅ)/abs(f‚Çñ)) >= cùêü)
            # println("on a modifi√© u")
            u = 0
            # k = k + 1
        end


        OK = update_and_stop!(nlp_stop, x = x‚Çñ, fx = f‚Çñ, gx = ‚àáf‚Çñ)

        # norm(‚àáf) bug: https://github.com/JuliaLang/julia/issues/11788
        ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
        k = k + 1

        verbose && @printf("%5d  %9.2e  %8.1e  %5d  %5d  %5d  %7.2e  %7.2e", k, nlp_at_x.fx, ‚àáfNorm, k, i, p, Œ±‚Çê, Œ±‚Çõ)

    end

    verbose && @printf("\n")

    return nlp_at_x, nlp_stop.meta.optimal
end
