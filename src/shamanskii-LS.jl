export Shamanskii_LS

"""
A globalized Shamanskii algorithm with Line Search.
This code is an implementation of the idea presented in:

    Global Convergence Technique for the Newton
        Method with Periodic Hessian Evaluation

By F. LAMPARIELLO and M. SCIANDRONE

JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS:
Vol. 111, No. 2, pp. 341‚Äì358, November 2001
"""
function Shamanskii_LS(nlp          :: AbstractNLPModel,
                 	   nlp_stop     :: NLPStopping;
                 	   linesearch   :: Function = armijo_ls,
                 	   Nwtdirection :: Function = NwtdirectionLDLT_LS,
                 	   hessian_rep  :: Function = hessian_dense,
					   Œ∑ 		    :: Float64 = 1.5,
					   ca           :: Float64 = 0.1,
					   cf           :: Float64 = 0.25,
					   u            :: Int = 0,
					   p            :: Int = 1,
                 	   kwargs...)

    nlp_at_x = nlp_stop.current_state
    n = nlp.meta.nvar
    xt = copy(nlp.meta.x0)
	T = eltype(xt)
    ‚àáft = Array{Float64}(undef, n)

    f = obj(nlp, xt)
    ‚àáf = grad(nlp, xt)
    OK = update_and_start!(nlp_stop, x = xt, fx = f, gx = ‚àáf, g0 = ‚àáf)
    ‚àáfNorm = norm(nlp_at_x.gx)

	global Hhat = hessian_rep(nlp, xt)
    global iter = 0
	global i = 0
	global fm1 = nothing

    @info log_header([:iter, :f, :nrm_g, :Œ±], [Int64, T, T, T])
	@info log_row(Any[iter, nlp_at_x.fx, ‚àáfNorm, 0.0])

    Œ≤ = 0.0
    d = zero(nlp_at_x.gx)
    h = LineSearch.LineModel(nlp, xt, d)

    while !OK
        d, u = Nwtdirection(Hhat, nlp_at_x.gx)
		@show u
        slope = d' * nlp_at_x.gx

        h = LineSearch.redirect!(h, xt, d)

        ls_at_t = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
        stop_ls = LS_Stopping(h, (x, y) -> armijo(x, y, œÑ‚ÇÄ = 1e-09), ls_at_t)
        ls_at_t, good_step_size = linesearch(h, stop_ls; kwargs...)
        xt  = nlp_at_x.x + ls_at_t.x * d
		fm1 = copy(nlp_at_x.fx)
        ft  = obj(nlp, xt); ‚àáft = grad(nlp, xt)

        ‚àáft = grad!(nlp, xt, ‚àáft)
		OK = update_and_stop!(nlp_stop, x = xt, fx = ft, gx = ‚àáft)

		# if (ls_at_t.x >= ca) || (((ft - fm1)/ft) >= cf)
		# 	u = 0
		# end

        # Move on.
		@show iter
		@show i * p
		@show u
		@show (iter == i * p)
		@show !(iter == i * p)
		if !(iter == i * p) && (u == 0)
			println("we do nothing")
		else
			println("we do something")
			Hhat = copy(hessian_rep(nlp, xt))
			if (iter == i * p)
				i += 1
			end
		end

        ‚àáfNorm = norm(nlp_at_x.gx)
        iter = iter + 1

		@info log_row(Any[iter, nlp_at_x.fx, ‚àáfNorm, ls_at_t.x])
    end

    return nlp_stop, nlp_stop.meta.optimal
end

# function Shamanskii_LS(nlp            :: AbstractNLPModel,
#                        nlp_stop       :: NLPStopping;
#                        linesearch     :: Function = armijo_ls,
#                        verbose        :: Bool = false,
#                        Nwtdirection   :: Function = NwtdirectionLDLt,
#                        hessian_rep    :: Function = hessian_dense,
#                        mem            :: Int = 2,
#                        Œ∑              :: Float64 = 1.5, # Œ∑ > 1
#                        c‚Çê             :: Float64 = 0.1, # c‚Çê ‚àà (0, 1]
#                        cùêü              :: Float64 = 0.25, # cùêü ‚àà (0, 1)
#                        p              :: Int = 10, # p > 1
#                        kwargs...)
#
#     nlp_at_x = nlp_stop.current_state
#
#     L = nothing; Q = nothing; Œì = nothing
#     D = nothing; pp = nothing; rho = nothing; ncomp = nothing;
#     Œ±‚Çê = 0.0; Œ±‚Çõ = 0.0;
#
#     x = copy(nlp.meta.x0) # our x‚ÇÄ
#     n = nlp.meta.nvar
#
#     # xt = Array{Float64}(n)
#     x‚Çñ = copy(nlp.meta.x0)
#     ‚àáf‚Çñ = Array{Float64}(undef, n)
#
#     f = obj(nlp, x)
#     f‚Çñ‚Çã‚ÇÅ = f
#     ‚àáf‚Çñ = grad(nlp, x)
#
#     # Initiliazation to avoir scope issues
#     H‚Çñ = nothing; H‚Çñ‚Çã‚ÇÅ = nothing; approx_H‚Çñ = nothing; approx_H‚Çñ‚Çã‚ÇÅ = nothing;
#     # Step 0 of the algorithm, initialize some parameters
#     k = 0; i = 0; u = 0
#
#     # Step 1 of the algorithm, we check if the initial point is stationnary
#     OK = update_and_start!(nlp_stop, x = x, fx = f, gx = ‚àáf‚Çñ, g0 = ‚àáf‚Çñ)
#
#
#     # ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
#     ‚àáfNorm = norm(nlp_at_x.gx)
#     # update!(nlp_at_x, Hx = hessian_rep(nlp, x))
#     # H‚Çñ‚Çã‚ÇÅ = nlp_at_x.Hx
#
#     verbose && @printf("%4s  %8s  %7s  %8s  %5s  %5s  %6s  %6s \n", " k", "f", "‚Äñ‚àáf‚Äñ", "k", "i", "p", "Œ±‚Çê", "Œ±‚Çõ")
#     verbose && @printf("%5d  %9.2e  %8.1e  %5d  %5d  %5d  %7.2e  %7.2e \n", k, nlp_at_x.fx, ‚àáfNorm, k, i, p, Œ±‚Çê, Œ±‚Çõ)
#
#     d = fill(0.0, size(nlp_at_x.gx)[1])
#
#
#     h = LineSearch.LineModel(nlp, x, d)
#
#     # Step 1 : We check if we have a stationnary point
#     while !OK #k < 10
#         # Step 2 : We compute the exact hessian
#         if (k == i * p) || (u == 1)   # !((k != i * p) && (u == 0))
#             # println("on est dans le if (k == i * p) || (u == 1)")
#             # Compute H‚Çñ and construct its positive definite approximation
#             # by applying the modified Cholesky factorization. Then u = 0 or
#             # u = 1 depending on wheter or not ÃÇH‚Çñ is to different fom H‚Çñ.
#             H‚Çñ = hessian_rep(nlp, x‚Çñ)
#             State.update!(nlp_at_x, Hx = H‚Çñ)
#
#             (L, D, pp, rho, ncomp) = ldlt_symm(H‚Çñ)
#             # println("On se rend ici ‚Üì")
#             # @show D
#             X = eigen(D)
#             # println("on a X ‚Üë")
#             Œî = X.values
#             Q =  X.vectors
#
#             œµ2 =  1.0e-8
#             Œì = max.(abs.(Œî),œµ2)
#             # sizeD = size(D)[1]^2
#             # for j = 1:sizeD
#             #     dj = D[j]
#             #     D[j] = max(dj, Œì)
#             # end
#
#
#             # println("on check si on a une bonne approximation de la hessienne avec Cholesky modifi√©")
#             # println("isposdef(H‚Çñ) = $(isposdef(H‚Çñ))")
#             good_hess_approx = hess_approx(H‚Çñ, L, D, Œ∑)
#             # println("k = $k and good_hess_approx = $good_hess_approx")
#             if good_hess_approx
#                 u = 0
#             else
#                 u = 1
#             end
#
#             # println("i = $i and p = $p")
#             if (k == i * p)
#                 i += 1
#             end
#         end # if (k == i * p) || (u == 1)
#
#         # Step 3 : ???
#         # ÃÇH‚Çñ = ÃÇH‚Çñ‚Çã‚ÇÅ
#
#         # Step 4: compute the descent direction and x‚Çñ‚Çä‚ÇÅ
#         # d = Nwtdirection(nlp_at_x.Hx, nlp_at_x.gx, verbose = false)
#         dÃÉ = L\nlp_at_x.gx[pp]
#         dÃÇ = L'\ (Q*(Q'*dÃÉ ./ Œì))
#         d = - dÃÇ[invperm(pp)]
#
#         slope = BLAS.dot(n, d, 1, nlp_at_x.gx, 1)
#
#         # verbose && @printf("  %8.1e", slope)
#
#         h = LineSearch.redirect!(h, x‚Çñ, d)
#
#         ls_at_t = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
#         stop_ls = LS_Stopping(h, (x, y) -> armijo(x, y, œÑ‚ÇÄ = 1e-09), ls_at_t)
#         ls_at_t, good_step_size = linesearch(h, stop_ls; kwargs...)
#
#         # ls_at_t = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
#         # ls_at_t1 = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
#         # ls_at_t2 = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
#         # stop_ls1 = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t1)
#         # stop_ls2 = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t2)
#         # verbose && println(" ")
#         # # ls_at_t, good_step_size = linesearch(h, stop_ls, LS_Function_Meta())
#         # ls_at_t1, good_step_size1 = shamanskii_line_search(h, stop_ls1, LS_Function_Meta())
#         # ls_at_t2, good_step_size2 = armijo_ls(h, stop_ls2, LS_Function_Meta())
#         # good_step_size1 || (nlp_stop.meta.stalled_linesearch = true)
#
#         Œ±‚Çõ = ls_at_t.x
#         # Œ±‚Çê = ls_at_t2.x
#         # println("good_step_size = $good_step_size  and Œ±‚Çñ = $Œ±‚Çñ")
#
#         # x‚Çñ = nlp_at_x.x + Œ±‚Çñ * d
#         x‚Çñ = nlp_at_x.x + Œ±‚Çõ * d
#         f‚Çñ = obj(nlp, x‚Çñ); ‚àáf‚Çñ = grad(nlp, x‚Çñ)
#
#         BLAS.blascopy!(n, nlp_at_x.x, 1, x‚Çñ, 1)
#         BLAS.axpy!(n, ls_at_t.x, d, 1, x‚Çñ, 1) #BLAS.axpy!(n, t, d, 1, xt, 1)
#         ‚àáf‚Çñ = grad!(nlp, x‚Çñ, ‚àáf‚Çñ)
#
#         # Step 5: We update the value of u
#         # if (Œ±‚Çñ >= c‚Çê) || (((f‚Çñ - f‚Çñ‚Çã‚ÇÅ)/abs(f‚Çñ)) >= cùêü)
#         if (Œ±‚Çõ >= c‚Çê) || (((f‚Çñ - f‚Çñ‚Çã‚ÇÅ)/abs(f‚Çñ)) >= cùêü)
#             # println("on a modifi√© u")
#             u = 0
#             # k = k + 1
#         end
#
#
#         OK = update_and_stop!(nlp_stop, x = x‚Çñ, fx = f‚Çñ, gx = ‚àáf‚Çñ)
#
#         # norm(‚àáf) bug: https://github.com/JuliaLang/julia/issues/11788
#         # ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
#         ‚àáfNorm = norm(nlp_at_x.gx)
#         k = k + 1
#
#         verbose && @printf("%5d  %9.2e  %8.1e  %5d  %5d  %5d  %7.2e  %7.2e \n", k, nlp_at_x.fx, ‚àáfNorm, k, i, p, Œ±‚Çê, Œ±‚Çõ)
#
#     end
#
#     verbose && @printf("\n")
#
#     return nlp_at_x, nlp_stop.meta.optimal
# end
