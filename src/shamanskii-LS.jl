export Shamanskii_LS

"""
A globalized Shamanskii algorithm with Line Search.
This code is an implementation of the idea presented in:

    Global Convergence Technique for the Newton
        Method with Periodic Hessian Evaluation

By F. LAMPARIELLO and M. SCIANDRONE

JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS:
Vol. 111, No. 2, pp. 341‚Äì358, November 2001
"""
function Shamanskii_LS(nlp            :: AbstractNLPModel,
                       nlp_stop       :: NLPStopping;
                       linesearch     :: Function = shamanskii_line_search,
                       verbose        :: Bool = false,
                       Nwtdirection   :: Function = NwtdirectionLDLT,
                       hessian_rep    :: Function = hessian_dense,
                       mem            :: Int = 2,
                       Œ∑              :: Float64 = 1.5, # Œ∑ > 1
                       c‚Çê             :: Float64 = 0.1, # c‚Çê ‚àà (0, 1]
                       cùêü              :: Float64 = 0.25, # cùêü ‚àà (0, 1)
                       p              :: Int = 10, # p > 1
                       kwargs...)

    # Data of the algorithm
    # Œ∑ = 1.5 # Œ∑ > 1
    # c‚Çê = 0.1 # c‚Çê ‚àà (0, 1]
    # cùêü = 0.25 # cùêü ‚àà (0, 1)
    # p = 10 # p > 1

    nlp_at_x = nlp_stop.current_state

    x = copy(nlp.meta.x0) # our x‚ÇÄ
    n = nlp.meta.nvar

    # xt = Array{Float64}(n)
    x‚Çñ = copy(nlp.meta.x0)
    ‚àáf‚Çñ = Array{Float64}(undef, n)

    f = obj(nlp, x)
    f‚Çñ‚Çã‚ÇÅ = f
    ‚àáf‚Çñ = grad(nlp, x)

    # Initiliazation to avoir scope issues
    H‚Çñ = nothing; H‚Çñ‚Çã‚ÇÅ = nothing; approx_H‚Çñ = nothing; approx_H‚Çñ‚Çã‚ÇÅ = nothing;
    # Step 0 of the algorithm, initialize some parameters
    k = 0; i = 0; u = 0

    # Step 1 of the algorithm, we check if the initial point is stationnary
    OK = update_and_start!(nlp_stop, x = x, fx = f, gx = ‚àáf‚Çñ, g0 = ‚àáf‚Çñ)


    ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
    # update!(nlp_at_x, Hx = hessian_rep(nlp, x))
    # H‚Çñ‚Çã‚ÇÅ = nlp_at_x.Hx

    verbose && @printf("%4s  %8s  %7s  %8s  \n", " k", "f", "‚Äñ‚àáf‚Äñ", "‚àáf'd")
    verbose && @printf("%5d  %9.2e  %8.1e", k, nlp_at_x.fx, ‚àáfNorm)

    d = fill(0.0, size(nlp_at_x.gx)[1])


    h = LineModel(nlp, x, d)

    # Step 1 : We check if we have a stationnary point
    while !OK #k < 10
        # Step 2 : We compute the exact hessian
        if (k == i * p) || (u == 1)   # !((k != i * p) && (u == 0))
            # Compute H‚Çñ and construct its positive definite approximation
            # by applying the modified Cholesky factorization. Then u = 0 or
            # u = 1 depending on wheter or not ÃÇH‚Çñ is to different fom H‚Çñ.
            H‚Çñ = hessian_rep(nlp, x)
            approx_H‚Çñ = ldl(H‚Çñ)
            println("on check si on a une bonne approximation de la hessienne avec Cholesky modifi√©")
            println("isposfed(H‚Çñ) = $(isposfed(H‚Çñ))")
            good_hess_approx = hess_approx(H‚Çñ, approx_H‚Çñ, Œ∑)
            println("k = $k and good_hess_approx = $good_hess_approx")
            if good_hess_approx
                u = 0
            else
                u = 1
            end

            println("i = $i and p = $p")
            if (k == i * p)
                i += 1
            end
        end

        # Step 3 : ???
        # ÃÇH‚Çñ = ÃÇH‚Çñ‚Çã‚ÇÅ

        # Step 4: compute the descent direction and x‚Çñ‚Çä‚ÇÅ
        # d = Nwtdirection(nlp_at_x.Hx, nlp_at_x.gx, verbose = false)
        d = approx_H‚Çñ \ -nlp_at_x.gx

        slope = BLAS.dot(n, d, 1, nlp_at_x.gx, 1)

        verbose && @printf("  %8.1e", slope)

        h = redirect!(h, x‚Çñ, d)

        ls_at_t = LSAtT(0.0, h‚ÇÄ = nlp_at_x.fx, g‚ÇÄ = slope)
        stop_ls = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t)
        verbose && println(" ")
        ls_at_t, good_step_size = linesearch(h, stop_ls, LS_Function_Meta())
        good_step_size || (nlp_stop.meta.stalled_linesearch = true)

        Œ±‚Çñ = ls_at_t.x
        println("good_step_size = $good_step_size  and Œ±‚Çñ = $Œ±‚Çñ")

        x‚Çñ = nlp_at_x.x + Œ±‚Çñ * d
        f‚Çñ = obj(nlp, x‚Çñ); ‚àáf‚Çñ = grad(nlp, x‚Çñ)

        BLAS.blascopy!(n, nlp_at_x.x, 1, x‚Çñ, 1)
        BLAS.axpy!(n, ls_at_t.x, d, 1, x‚Çñ, 1) #BLAS.axpy!(n, t, d, 1, xt, 1)
        ‚àáf‚Çñ = grad!(nlp, x‚Çñ, ‚àáf‚Çñ)

        # Step 5: We update the value of u
        if (Œ±‚Çñ >= c‚Çê) || (((f‚Çñ - f‚Çñ‚Çã‚ÇÅ)/abs(f‚Çñ)) >= cùêü)
            println("on a modifi√© u")
            u = 0
            # k = k + 1
        end


        OK = update_and_stop!(nlp_stop, x = x‚Çñ, fx = f‚Çñ)
        BLAS.blascopy!(n, ‚àáf‚Çñ, 1, nlp_at_x.gx, 1)

        # norm(‚àáf) bug: https://github.com/JuliaLang/julia/issues/11788
        ‚àáfNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
        k = k + 1

        verbose && @printf("%4d  %9.2e  %8.1e ", k, nlp_at_x.fx, ‚àáfNorm)

    end

    verbose && @printf("\n")

    return nlp_at_x, nlp_stop.meta.optimal
end
