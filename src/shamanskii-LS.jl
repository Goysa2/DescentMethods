export Shamanskii_LS

"""
A globalized Shamanskii algorithm with Line Search.
This code is an implementation of the idea presented in:

    Global Convergence Technique for the Newton
        Method with Periodic Hessian Evaluation

By F. LAMPARIELLO and M. SCIANDRONE

JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS:
Vol. 111, No. 2, pp. 341â€“358, November 2001
"""
function Shamanskii_LS(nlp            :: AbstractNLPModel,
                       nlp_stop       :: NLPStopping;
                       linesearch     :: Function = shamanskii_line_search,
                       verbose        :: Bool = false,
                       Nwtdirection   :: Function = NwtdirectionLDLT,
                       hessian_rep    :: Function = hessian_dense,
                       mem            :: Int = 2,
                       kwargs...)

    # Data of the algorithm
    Î· = 1.5 # Î· > 1
    câ‚ = 0.1 # câ‚ âˆˆ (0, 1]
    cğŸ = 0.25 # cğŸ âˆˆ (0, 1)
    p = 10 # p > 1

    nlp_at_x = nlp_stop.current_state

    x = copy(nlp.meta.x0) # our xâ‚€
    n = nlp.meta.nvar

    # xt = Array{Float64}(n)
    xt = copy(nlp.meta.x0)
    âˆ‡ft = Array{Float64}(undef, n)

    f = obj(nlp, x)
    fâ‚–â‚‹â‚ = f
    âˆ‡fâ‚– = grad(nlp, x)

    # Initiliazation to avoir scope issues
    Hâ‚– = nothing; Hâ‚–â‚‹â‚ = nothing; approx_Hâ‚– = nothing; approx_Hâ‚–â‚‹â‚ = nothing;
    # Step 0 of the algorithm, initialize some parameters
    k = 0; i = 0; u = 0

    # Step 1 of the algorithm, we check if the initial point is stationnary
    OK = update_and_start!(nlp_stop, x = x, fx = f, gx = âˆ‡fâ‚–, g0 = âˆ‡fâ‚–)


    âˆ‡fNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
    # update!(nlp_at_x, Hx = hessian_rep(nlp, x))
    # Hâ‚–â‚‹â‚ = nlp_at_x.Hx

    verbose && @printf("%4s  %8s  %7s  %8s  \n", " k", "f", "â€–âˆ‡fâ€–", "âˆ‡f'd")
    verbose && @printf("%5d  %9.2e  %8.1e", k, nlp_at_x.fx, âˆ‡fNorm)

    d = fill(0.0, size(nlp_at_x.gx)[1])


    h = LineModel(nlp, x, d)

    # Step 1 : We check if we have a stationnary point
    while !OK
        # Step 2 : We compute the exact hessian
        if (k == i * p) || (u == 1)   # (k != i * p) && (u == 0)
            # Compute Hâ‚– and construct its positive definite approximation
            # by applying the modified Cholesky factorization. Then u = 0 or
            # u = 1 depending on wheter or not Ì‚Hâ‚– is to different fom Hâ‚–.
            Hâ‚– = hessian_rep(nlp, x)
            approx_Hâ‚– = ldl(Hâ‚–)
            good_hess_approx = hess_approx(Hâ‚–, approx_Hâ‚–, Î·)
            if good_hess_approx
                u = 1
            else
                u = 0
            end

            if (k == i * p)
                i += 1
            end
        end

        # Step 3 : ???
        # Ì‚Hâ‚– = Ì‚Hâ‚–â‚‹â‚

        # Step 4: compute the descent direction and xâ‚–â‚Šâ‚
        # d = Nwtdirection(nlp_at_x.Hx, nlp_at_x.gx, verbose = false)
        d = approx_Hâ‚– \ -nlp_at_x.gx
        slope = BLAS.dot(n, d, 1, nlp_at_x.gx, 1)

        verbose && @printf("  %8.1e", slope)

        h = redirect!(h, xt, d)

        ls_at_t = LSAtT(0.0, hâ‚€ = nlp_at_x.fx, gâ‚€ = slope)
        stop_ls = LS_Stopping(h, (x, y) -> shamanskii_stop(x, y), ls_at_t)
        verbose && println(" ")
        ls_at_t, good_step_size = linesearch(h, stop_ls, LS_Function_Meta())
        good_step_size || (nlp_stop.meta.stalled_linesearch = true)

        Î±â‚– = ls_at_t.x

        xâ‚– = nlp_at_x.x + Î±â‚– * d
        fâ‚– = obj(nlp, xt); âˆ‡fâ‚– = grad(nlp, xâ‚–)

        BLAS.blascopy!(n, nlp_at_x.x, 1, xâ‚–, 1)
        BLAS.axpy!(n, ls_at_t.x, d, 1, xâ‚–, 1) #BLAS.axpy!(n, t, d, 1, xt, 1)
        âˆ‡ft = grad!(nlp, xâ‚–, âˆ‡fâ‚–)

        # Step 5: We update the value of u
        if (Î±â‚– >= câ‚) || (((fâ‚– - fâ‚–â‚‹â‚)/abs(fâ‚–)) >= cğŸ)
            u = 0
        end


        OK = update_and_stop!(nlp_stop, x = xâ‚–, fx = fâ‚–)
        BLAS.blascopy!(n, âˆ‡ft, 1, nlp_at_x.gx, 1)

        # norm(âˆ‡f) bug: https://github.com/JuliaLang/julia/issues/11788
        âˆ‡fNorm = BLAS.nrm2(n, nlp_at_x.gx, 1)
        k = k + 1

        verbose && @printf("%4d  %9.2e  %8.1e ", k, nlp_at_x.fx, âˆ‡fNorm)

    end

    verbose && @printf("\n")

    return nlp_at_x, nlp_stop.meta.optimal
end
